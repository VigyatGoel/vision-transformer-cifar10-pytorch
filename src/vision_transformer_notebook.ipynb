{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd6484b",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) on CIFAR-10\n",
    "\n",
    "This notebook demonstrates how to implement and train a Vision Transformer (ViT) from scratch using PyTorch on the CIFAR-10 dataset.\n",
    "\n",
    "- **Author:** [Your Name]\n",
    "- **Date:** April 2025\n",
    "- **Dataset:** CIFAR-10 (60,000 32x32 color images in 10 classes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df7c92",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "We start by importing PyTorch, torchvision, and other necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66322a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c4be15",
   "metadata": {},
   "source": [
    "## 2. Vision Transformer Components\n",
    "Below are the core building blocks for the Vision Transformer: Patch Embedding, Embedding Layer, Transformer Encoder Block, and the full ViT model.\n",
    "Each class is explained in comments within the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24038e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEmbeddings(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22428390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        \n",
    "        attention_output, _ = self.attention(x_norm, x_norm, x_norm)\n",
    "        x = x + self.dropout(attention_output)\n",
    "        \n",
    "        x_norm = self.norm2(x)\n",
    "        ffn_output = self.feed_forward(x_norm)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43638a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_dim=mlp_dim,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embeddings = ViTEmbeddings(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_layers,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.encoder(x)\n",
    "        cls_token_output = x[:, 0]\n",
    "        logits = self.cls_head(cls_token_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac048876",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Training Utilities\n",
    "We define the data transforms, training, and evaluation functions.\n",
    "- **Transform:** Resizes images, converts to tensor, and normalizes.\n",
    "- **train:** Trains the model for a given number of epochs.\n",
    "- **evaluate:** Evaluates the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04487a",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation\n",
    "We load the CIFAR-10 dataset, initialize the Vision Transformer, and train it for one epoch. The model is then evaluated on the test set.\n",
    "\n",
    "**Note:** For demonstration, training is set to 10 epoch. Increase epochs for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c87ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    img_size = 32\n",
    "    patch_size = 4\n",
    "    num_patches = (img_size // patch_size) ** 2\n",
    "    \n",
    "    model = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        in_channels=3,\n",
    "        embed_dim=128,\n",
    "        num_heads=4,\n",
    "        mlp_dim=256,\n",
    "        num_layers=6,\n",
    "        num_classes=10,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "    train(model, train_loader, criterion, optimizer, device, epochs=1)\n",
    "    evaluate(model, test_loader, device)\n",
    "\n",
    "    torch.save(model, \"vit_cifar10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1646d1",
   "metadata": {},
   "source": [
    "## 5. Results and Model Saving\n",
    "After training, the test accuracy is printed and the model is saved to `vit_cifar10.pth`.\n",
    "\n",
    "You can now use this notebook as a template for your own ViT experiments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
